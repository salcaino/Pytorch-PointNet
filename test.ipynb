{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32])\n"
     ]
    }
   ],
   "source": [
    "b = 2\n",
    "k = 16\n",
    "n = 50\n",
    "m = nn.AdaptiveMaxPool1d(b*k)\n",
    "input = torch.randn(b, k, n)\n",
    "\n",
    "output = m(input)[0]\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batchSize', type=int, default=32, help='input batch size')\n",
    "parser.add_argument('--num_points', type=int, default=2500, help='input batch size')\n",
    "parser.add_argument('--workers', type=int, help='number of data loading workers', default=4)\n",
    "parser.add_argument('--model', type=str, default='', help='model path')\n",
    "parser.add_argument('--nepoch', type=int, default=250, help='number of epochs to train for')\n",
    "parser.add_argument('--outf', type=str, default='cls', help='output folder')\n",
    "parser.add_argument('--dataset', type=str, required=True, help=\"dataset path\")\n",
    "parser.add_argument('--feature_transform', default='True', help=\"use feature transform\")\n",
    "parser.add_argument('--save_dir', default='../pretrained_networks', help='directory to save model weights')\n",
    "\n",
    "opt = parser.parse_args()\n",
    "print(opt)\n",
    "\n",
    "if not os.path.exists(opt.save_dir):\n",
    "    os.mkdir(opt.save_dir, exist_ok=True)\n",
    "try:\n",
    "    os.makedirs(opt.outf, exist_ok=True)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "\n",
    "opt.manualSeed = random.randint(1, 10000)  # fix seed\n",
    "print(\"Random Seed: \", opt.manualSeed)\n",
    "random.seed(opt.manualSeed)\n",
    "torch.manual_seed(opt.manualSeed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = ShapeNetDataset(\n",
    "    root=opt.dataset,\n",
    "    classification=True,\n",
    "    npoints=opt.num_points)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=opt.batchSize,\n",
    "    shuffle=True,\n",
    "    num_workers=int(opt.workers))\n",
    "\n",
    "val_dataset = ShapeNetDataset(\n",
    "    root=opt.dataset,\n",
    "    classification=True,\n",
    "    split='val',\n",
    "    npoints=opt.num_points)\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=opt.batchSize,\n",
    "    shuffle=False,\n",
    "    num_workers=int(opt.workers))\n",
    "\n",
    "print(len(dataset))\n",
    "num_classes = len(dataset.classes)\n",
    "print('classes', num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = PointNetCls(num_classes=num_classes, feature_transform=opt.feature_transform)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5, verbose=False)\n",
    "\n",
    "best_acc = -1\n",
    "epochs = 0\n",
    "\n",
    "if opt.model != '':\n",
    "    # classifier.load_state_dict(torch.load(opt.model))\n",
    "  # model.load_state_dict(torch.load(path))\n",
    "  checkpoint = torch.load(opt.model)\n",
    "  classifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "  best_acc = checkpoint['best_acc']\n",
    "  epochs = checkpoint['epochs']\n",
    "  print(f'Model loaded with best acc of {best_acc}')\n",
    "\n",
    "classifier.to(device)\n",
    "\n",
    "num_batch = len(dataloader)\n",
    "epoch_losses = []\n",
    "epoch_accuracy = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs, epochs + opt.nepoch):\n",
    "    classifier.train()\n",
    "    epoch_loss = []\n",
    "    pbar = tqdm(enumerate(dataloader), desc='Batches', leave = False)\n",
    "    for i, data in pbar:\n",
    "        points, target = data\n",
    "        target = target[:, 0]\n",
    "        points = points.transpose(2, 1)\n",
    "        points, target = points.to(device), target.to(device)\n",
    "        \n",
    "        # perform forward and backward paths, optimize network\n",
    "        scores, trans, trans_feat = classifier(points)\n",
    "        loss = criterion(scores, target) \n",
    "        loss = loss + feature_transform_regularizer(trans) + feature_transform_regularizer(trans_feat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tloss = loss.item()\n",
    "        epoch_loss.append(tloss)\n",
    "        pbar.set_description(f\"loss: {tloss}\")\n",
    "    \n",
    "    classifier.eval()\n",
    "    total_preds = []\n",
    "    total_targets = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_dataloader, 0):\n",
    "            points, target = data\n",
    "            target = target[:, 0]\n",
    "            points = points.transpose(2, 1)\n",
    "            points, target = points.to(device), target.to(device)\n",
    "\n",
    "            preds, _, _ = classifier(points)\n",
    "            pred_labels = torch.max(preds, dim= 1)[1]\n",
    "\n",
    "            total_preds = np.concatenate([total_preds, pred_labels.cpu().numpy()])\n",
    "            total_targets = np.concatenate([total_targets, target.cpu().numpy()])\n",
    "            a = 0\n",
    "        accuracy = 100 * (total_targets == total_preds).sum() / len(val_dataset)\n",
    "        print('Accuracy = {:.2f}%'.format(accuracy))\n",
    "        epoch_accuracy.append(accuracy)\n",
    "    accuracy = round(accuracy, 3)\n",
    "    if accuracy > best_acc or best_acc < 0:\n",
    "        best_acc = accuracy\n",
    "        print(f\"Saving new best model with best acc {best_acc}\")\n",
    "        path = os.path.join(opt.save_dir, f'latest_classification_feat_trans_{opt.feature_transform}.pt')\n",
    "        torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': classifier.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_acc' : best_acc\n",
    "                }, path)\n",
    "    # torch.save({'model':classifier.state_dict(),\n",
    "    #             'optimizer': optimizer.state_dict(),\n",
    "    #             'epoch': epoch}, os.path.join(opt.save_dir, 'latest_classification.pt'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2d2abdcfbca0a4268917a0dd2311249d9c685940543b2ee1d931ae69dee20be"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 ('bigdata')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
